# 开题报告参考文献
## CV领域适应
### 原始论文 2016年
Ganin, Yaroslav, et al. "Domain-adversarial training of neural networks." The Journal of Machine Learning Research 17.1 (2016): 2096-2030.
### 被引用论文
暂时省略
### 2016年之后的引用论文

- Tzeng, Eric, et al. "Adversarial discriminative domain adaptation." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.

  **Abstract:** Adversarial learning methods are a promising approach to training robust deep networks, and can generate complex samples across diverse domains. They can also improve recognition despite the presence of domain shift or dataset bias: recent adversarial approaches to unsupervised domain adaptation reduce the difference between the training and test domain distributions and thus improve generalization performance. However, while generative adversarial net-works (GANs) show compelling visualizations, they are not optimal on discriminative tasks and can be limited to smaller shifts. On the other hand, discriminative approaches can handle larger domain shifts, but impose tied weights on the model and do not exploit a GAN-based loss. In this work, we first outline a novel generalized framework for adver-sarial adaptation, which subsumes recent state-of-the-art approaches as special cases, and use this generalized view to better relate prior approaches. We then propose a previ-ously unexplored instance of our general framework which combines discriminative modeling, untied weight sharing, and a GAN loss, which we call Adversarial Discriminative Domain Adaptation (ADDA). We show that ADDA is more effective yet considerably simpler than competing domain-adversarial methods, and demonstrate the promise of our approach by exceeding state-of-the-art unsupervised adapta-tion results on standard domain adaptation tasks as well as a difficult cross-modality object classification task. 

- Bousmalis, Konstantinos, et al. "Domain separation networks." Advances in Neural Information Processing Systems. 2016.

  **Abstract:** The cost of large scale data collection and annotation often makes the application of machine learning algorithms to new tasks or datasets prohibitively expensive. One approach circumventing this cost is training models on synthetic data where annotations are provided automatically. Despite their appeal, such models often fail to generalize from synthetic to real images, necessitating domain adaptation algorithms to manipulate these models before they can be successfully applied. Existing approaches focus either on mapping representations from one domain to the other, or on learning to extract features that are invariant to the domain from which they were extracted. However, by focusing only on creating a mapping or shared representation between the two domains, they ignore the individual characteristics of each domain. We hypothesize that explicitly modeling what is unique to each domain can improve a model's ability to extract domain-invariant features. Inspired by work on private-shared component analysis, we explicitly learn to extract image representations that are partitioned into two subspaces: one component which is private to each domain and one which is shared across domains. Our model is trained to not only perform the task we care about in the source domain, but also to use the partitioned representation to reconstruct the images from both domains. Our novel architecture results in a model that outperforms the state-of-the-art on a range of unsupervised domain adaptation scenarios and additionally produces visualizations of the private and shared representations enabling interpretation of the domain adaptation process.

- Saito, Kuniaki, Yoshitaka Ushiku, and Tatsuya Harada. "Asymmetric tri-training for unsupervised domain adaptation." Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.

  **Abstract:** Deep-layered models trained on a large number of labeled samples boost the accuracy of many tasks. It is important to apply such models to dif-ferent domains because collecting many labeled samples in various domains is expensive. In un-supervised domain adaptation, one needs to train a classifier that works well on a target domain when provided with labeled source samples and unlabeled target samples. Although many meth-ods aim to match the distributions of source and target samples, simply matching the distribution cannot ensure accuracy on the target domain. To learn discriminative representations for the target domain, we assume that artificially labeling tar-get samples can result in a good representation. Tri-training leverages three classifiers equally to give pseudo-labels to unlabeled samples, but the method does not assume labeling samples gener-ated from a different domain. In this paper, we propose an asymmetric tri-training method for unsupervised domain adaptation, where we as-sign pseudo-labels to unlabeled samples and train neural networks as if they are true labels. In our work, we use three networksasymmetrically. By asymmetric, we mean that two networks are used to label unlabeled target samples and one net-work is trained by the samples to obtain target-discriminative representations. We evaluate our method on digit recognition and sentiment anal-ysis datasets. Our proposed method achieves state-of-the-art performance on the benchmark digit recognition datasets of domain adaptation.

- Haeusser, Philip, et al. "Associative domain adaptation." Proceedings of the IEEE International Conference on Computer Vision. 2017.

  **Abstract:** We propose "associative domain adaptation", a novel technique for end-to-end domain adaptation with neural networks, the task of inferring class labels for an unlabeled target domain based on the statistical properties of a labeled source domain. Our training scheme follows the paradigm that in order to effectively derive class labels for the target domain, a network should produce statistically domain invariant embeddings, while minimizing the classification error on the labeled source domain. We accomplish this by reinforcing "associations" between source and target data directly in embedding space. Our method can easily be added to any existing classification network with no structural and almost no computational overhead. We demonstrate the effectiveness of our approach on various benchmarks and achieve state-of-the-art results across the board with a generic convolutional neural network architecture not specifically tuned to the respective tasks. Finally, we show that the proposed association loss produces embeddings that are more effective for domain adaptation compared to methods employing maximum mean discrepancy as a similarity measure in embedding space.

- Luo, Zelun, et al. "Label efficient learning of transferable representations acrosss domains and tasks." Advances in Neural Information Processing Systems. 2017.

  **Abstract:** We propose a framework that learns a representation transferable across different domains and tasks in a data efficient manner. Our approach battles domain shift with a domain adversarial loss, and generalizes the embedding to novel task using a metric learning-based approach. Our model is simultaneously optimized on labeled source data and unlabeled or sparsely labeled data in the target domain. Our method shows compelling results on novel classes within a new domain even when only a few labeled examples per class are available, outperforming the prevalent fine-tuning approach. In addition, we demonstrate the effectiveness of our framework on the transfer learning task from image object recognition to video action recognition.

- Cariucci, Fabio Maria, et al. "Autodial: Automatic domain alignment layers." 2017 IEEE International Conference on Computer Vision (ICCV). IEEE, 2017.

  **Abstract:** Classifiers trained on given databases perform poorly when tested on data acquired in different settings. This is explained in domain adaptation through a shift among distributions of the source and target domains. Attempts to align them have traditionally resulted in works reducing the domain shift by introducing appropriate loss terms, measuring the discrepancies between source and target distributions, in the objective function. Here we take a different route, proposing to align the learned representations by embedding in any given network specific Domain Alignment Layers, designed to match the source and target feature distributions to a reference one. Opposite to previous works which define a priori in which layers adaptation should be performed, our method is able to automatically learn the degree of feature alignment required at different levels of the deep network. Thorough experiments on different public benchmarks, in the unsupervised setting, confirm the power of our approach.

- Motiian, Saeid, et al. "Few-shot adversarial domain adaptation." Advances in Neural Information Processing Systems. 2017.

  **Abstract:** This work provides a framework for addressing the problem of supervised domain adaptation with deep models. The main idea is to exploit adversarial learning to learn an embedded subspace that simultaneously maximizes the confusion between two domains while semantically aligning their embedding. The supervised setting becomes attractive especially when there are only a few target data samples that need to be labeled. In this few-shot learning scenario, alignment and separation of semantic probability distributions is difficult because of the lack of data. We found that by carefully designing a training scheme whereby the typical binary adversarial discriminator is augmented to distinguish between four different classes, it is possible to effectively address the supervised adaptation problem. In addition, the approach has a high “speed” of adaptation, i.e. it requires an extremely low number of labeled target training samples, even one per category can be effective. We then extensively compare this approach to the state of the art in domain adaptation in two experiments: one using datasets for handwritten digit recognition, and one using datasets for visual object recognition.

## 其他领域适应论文

- McClosky, David. "Any domain parsing: automatic domain adaptation for natural language parsing." (2010).

  **Abstract:** Current efforts in syntactic parsing are largely data-driven. These methods require labeled examples of syntactic structures to learn statistical patterns governing these structures. Labeled data typically requires expert annotators which makes it both time consuming and costly to produce. Furthermore, once training data has been created for one textual domain, portability to similar domains is limited. This domain-dependence has inspired a large body of work since syntactic parsing aims to capture syntactic patterns across an entire language rather than just a specific domain.
The simplest approach to this task is to assume that the target domain is essentially the same as the source domain. No additional knowledge about the target domain is included. A more realistic approach assumes that only raw text from the target domain is available. This assumption lends itself well to semi-supervised learning methods since these utilize both labeled and unlabeled examples.
This dissertation focuses on a family of semi-supervised methods called self-training. Self-training creates semi-supervised learners from existing supervised learners with minimal effort. We first show results on self-training for constituency parsing within a single domain. While self-training has failed here in the past, we present a simple modification which allows it to succeed, producing state-of-the-art results for English constituency parsing. Next, we show how self-training is beneficial when parsing across domains and helps further when raw text is available from the target domain. One of the remaining issues is that one must choose a training corpus appropriate for the target domain or performance may be severely impaired. Humans can do this in some situations, but this strategy becomes less practical as we approach larger data sets. We present a technique, Any Domain Parsing, which automatically detects useful source domains and mixes them together to produce a customized parsing model. The resulting models perform almost as well as the best seen parsing models (oracle) for each target domain. As a result, we have a fully automatic syntactic constituency parser which can produce high-quality parses for all types of text, regardless of domain.

- Conneau, Alexis, et al. "Word translation without parallel data." arXiv preprint arXiv:1710.04087 (2017).

  **Abstract:** State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character informa-tion, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available

- Chen, Xinchi, et al. "Adversarial multi-criteria learning for chinese word segmentation." arXiv preprint arXiv:1704.07556 (2017).

  **Abstract:** Different linguistic perspectives causes many diverse segmentation criteria for Chinese word segmentation (CWS). Most existing methods focus on improve the performance for each single criterion. However, it is interesting to exploit these different criteria and mining their com-mon underlying knowledge. In this pa-per, we propose adversarial multi-criteria learning for CWS by integrating shared knowledge from multiple heterogeneous segmentation criteria. Experiments on eight corpora with heterogeneous segmen-tation criteria show that the performance of each corpus obtains a significant im-provement, compared to single-criterion learning. Source codes of this paper are available on Github.
- Zhang, Meng, et al. "Adversarial training for unsupervised bilingual lexicon induction." Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2017.

  **Abstract:** Word embeddings are well known to cap-ture linguistic regularities of the language on which they are trained. Researchers also observe that these regularities can transfer across languages. However, previ-ous endeavors to connect separate mono-lingual word embeddings typically require cross-lingual signals as supervision, either in the form of parallel corpus or seed lex-icon. In this work, we show that such cross-lingual connection can actually be established without any form of supervi-sion. We achieve this end by formulating the problem as a natural adversarial game, and investigating techniques that are cru-cial to successful training. We carry out evaluation on the unsupervised bilingual lexicon induction task. Even though this task appears intrinsically cross-lingual,we are able to demonstrate encouraging per-formance without any cross-lingual clues. 

- Mou, Lili, et al. "How transferable are neural networks in nlp applications?." arXiv preprint arXiv:1603.06111 (2016).

  **Abstract:** Transfer learning is aimed to make use of valuable knowledge in a source domain to help model performance in a target domain. It is particularly important to neural networks, which are very likely to be overfitting. In some fields like image processing, many studies have shown the effectiveness of neural network-based transfer learning. For neural NLP, however, existing studies have only casually applied transfer learning, and conclusions are inconsistent. In this paper, we conduct systematic case studies and provide an illuminating picture on the transferability of neural networks in NLP.

- Ruder, Sebastian, and Barbara Plank. "Strong baselines for neural semi-supervised learning under domain shift." arXiv preprint arXiv:1804.09530 (2018).

  **Abstract:** Novel neural models have been proposed in recent years for learning under domain shift. Most models, however, only evaluate on a single task, on proprietary datasets, or compare to weak baselines, which makes comparison of models difficult. In this paper, we re-evaluate classic general-purpose bootstrapping approaches in the context of neural networks under domain shifts vs. recent neural approaches and propose a novel multi-task tri-training method that reduces the time and space complexity of classic tri-training. Extensive experiments on two benchmarks are negative: while our novel method establishes a new state-of-the-art for sentiment analysis, it does not fare consistently the best. More importantly, we arrive at the somewhat surprising conclusion that classic tri-training, with some additions, outperforms the state of the art. We conclude that classic approaches constitute an important and strong baseline.

- Zhang, Yuan, Regina Barzilay, and Tommi Jaakkola. "Aspect-augmented adversarial networks for domain adaptation." Transactions of the Association for Computational Linguistics 5 (2017): 515-528.

  **Abstract:** We introduce a neural method for transfer learning between two (source and target) classification tasks or aspects over the same domain. Rather than training on target labels, we use a few keywords pertaining to source and target aspects indicating sentence relevance instead of document class labels. Documents are encoded by learning to embed and softly select relevant sentences in an aspect-dependent manner. A shared classifier is trained on the source encoded documents and labels, and applied to target encoded documents. We ensure transfer through aspect-adversarial training so that encoded documents are, as sets, aspect-invariant. Experimental results demonstrate that our approach outperforms different baselines and model variants on two datasets, yielding an improvement of 27% on a pathology dataset and 5% on a review dataset.

## ACL2019
- Semi-supervised Domain Adaptation for Dependency Parsing
  Zhenghua Li, Xue Peng, Min Zhang, Rui Wang and Luo Si

- Task Refinement Learning for Improved Accuracy and Stability of Unsupervised Domain Adaptation
  Yftah Ziser and Roi Reichart

- Automatic Domain Adaptation Outperforms Manual Domain Adaptation for Predicting Financial Outcomes
  Marina Sedinkina, Nikolas Breitkopf and Hinrich Schütze

- Reinforced Training Data Selection for Domain Adaptation
  Miaofeng Liu, Yan Song, Hongbin Zou and Tong Zhang

- Automatic Generation of High Quality CCGbanks for Parser Domain Adaptation
  Masashi Yoshikawa, Hiroshi Noji, Koji Mineshima and Daisuke Bekki

- A Cross-Domain Transferable Neural Coherence Model
  Peng Xu, Hamidreza Saghir, Jin Sung Kang, Teng Long, Avishek Joey Bose, Yanshuai Cao and Jackie Chi Kit Cheung

- Semi-supervised Stochastic Multi-Domain Learning using Variational Inference
  Yitong Li, Timothy Baldwin and Trevor Cohn

- Dynamically Composing Domain-Data Selection with Clean-Data Selection by ``Co-Curricular Learning" for Neural Machine Translation
  Wei Wang, Isaac Caswell and Ciprian Chelba

- Neural Temporality Adaptation for Document Classification: Diachronic Word Embeddings and Domain Adaptation Models
  Xiaolei Huang and Michael J. Paul

- SParC: Cross-Domain Semantic Parsing in Context
  Tao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria Lin, Suyi Li, Heyang Er, Irene Li, Bo Pang, Tao Chen, Emily Ji, Shreya Dixit, David Proctor, Sungrok Shim, Jonathan Kraft, Vincent Zhang, Caiming Xiong, Richard Socher and Dragomir Radev

- Cross-domain and Cross-lingual Abusive Language Detection: a Hybrid Approach with Deep Learning and a Multilingual Lexicon
  Endang Wahyu Pamungkas and Viviana Patti

- Robust Zero-Shot Cross-Domain Slot Filling with Example Values
  Darsh Shah, Raghav Gupta, Amir Fayazi and Dilek Hakkani-Tur

- Reversing Gradients in Adversarial Domain Adaptation for Question Deduplication and Textual Entailment Tasks
  Anush Kamath, Sparsh Gupta and Vitor Carvalho

- Domain Adaptive Inference for Neural Machine Translation
  Danielle Saunders, Felix Stahlberg, Adrià de Gispert and Bill Byrne

- Adversarial Domain Adaptation Using Artificial Titles for Abstractive Title Generation
  Francine Chen and Yan-Ying Chen

- Domain Adaptation of Neural Machine Translation by Lexicon Induction
  Junjie Hu, Mengzhou Xia, Graham Neubig and Jaime Carbonell

- Cross-Domain NER using Cross-Domain Language Modeling
  Chen Jia, Xiaobo Liang and Yue Zhang

- Towards Complex Text-to-SQL in Cross-Domain Database with Intermediate Representation
  Jiaqi Guo, Zecheng Zhan, Yan Gao, Yan Xiao, Jian-Guang LOU, Ting Liu and Dongmei Zhang

- Handling Domain Shift in Coreference Evaluation by Using Automatically Extracted Minimum Spans
  Nafise Sadat Moosavi, Leo Born, Massimo Poesio and Michael Strube

- Domain Adaptive Dialog Generation via Meta Learning
  Kun Qian and Zhou Yu

- Multi-Source Cross-Lingual Model Transfer: Learning What to Share
  Xilun Chen, Ahmed Hassan Awadallah, Hany Hassan, Wei Wang and Claire Cardie

- Dual Adversarial Neural Transfer for Low-Resource Named Entity Recognition
  Joey Tianyi Zhou, Hao Zhang, Di Jin, Hongyuan Zhu, Meng Fang, Rick Siow Mong Goh and Kenneth Kwok

- Learning Transferable Feature Representations Using Neural Networks
  Himanshu Sharad Bhatt, Shourya Roy, Arun Rajkumar and Sriranjani Ramakrishnan

- Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies
  Yunsu Kim, Yingbo Gao and Hermann Ney

- Low-resource Deep Entity Resolution with Transfer and Active Learning
  Jungo Kasai, Kun Qian, Sairam Gurajada, Yunyao Li and Lucian Popa

- Massively Multilingual Transfer for NER
  Afshin Rahimi, Yuan Li and Trevor Cohn

- Cross-Lingual Syntactic Transfer through Unsupervised Adaptation of Invertible Projections
  Junxian He, Zhisong Zhang, Taylor Berg-Kirkpatrick and Graham Neubig

- 



























