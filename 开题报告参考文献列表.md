# 开题报告参考文献
## CV领域适应
### 原始论文 2016年
Ganin, Yaroslav, et al. "Domain-adversarial training of neural networks." The Journal of Machine Learning Research 17.1 (2016): 2096-2030.
### 被引用论文
暂时省略
### 2016年之后的引用论文

- Tzeng, Eric, et al. "Adversarial discriminative domain adaptation." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.

  **Abstract:** Adversarial learning methods are a promising approach to training robust deep networks, and can generate complex samples across diverse domains. They can also improve recognition despite the presence of domain shift or dataset bias: recent adversarial approaches to unsupervised domain adaptation reduce the difference between the training and test domain distributions and thus improve generalization performance. However, while generative adversarial net-works (GANs) show compelling visualizations, they are not optimal on discriminative tasks and can be limited to smaller shifts. On the other hand, discriminative approaches can handle larger domain shifts, but impose tied weights on the model and do not exploit a GAN-based loss. In this work, we first outline a novel generalized framework for adver-sarial adaptation, which subsumes recent state-of-the-art approaches as special cases, and use this generalized view to better relate prior approaches. We then propose a previ-ously unexplored instance of our general framework which combines discriminative modeling, untied weight sharing, and a GAN loss, which we call Adversarial Discriminative Domain Adaptation (ADDA). We show that ADDA is more effective yet considerably simpler than competing domain-adversarial methods, and demonstrate the promise of our approach by exceeding state-of-the-art unsupervised adapta-tion results on standard domain adaptation tasks as well as a difficult cross-modality object classification task. 

- Bousmalis, Konstantinos, et al. "Domain separation networks." Advances in Neural Information Processing Systems. 2016.

  **Abstract:** The cost of large scale data collection and annotation often makes the application of machine learning algorithms to new tasks or datasets prohibitively expensive. One approach circumventing this cost is training models on synthetic data where annotations are provided automatically. Despite their appeal, such models often fail to generalize from synthetic to real images, necessitating domain adaptation algorithms to manipulate these models before they can be successfully applied. Existing approaches focus either on mapping representations from one domain to the other, or on learning to extract features that are invariant to the domain from which they were extracted. However, by focusing only on creating a mapping or shared representation between the two domains, they ignore the individual characteristics of each domain. We hypothesize that explicitly modeling what is unique to each domain can improve a model's ability to extract domain-invariant features. Inspired by work on private-shared component analysis, we explicitly learn to extract image representations that are partitioned into two subspaces: one component which is private to each domain and one which is shared across domains. Our model is trained to not only perform the task we care about in the source domain, but also to use the partitioned representation to reconstruct the images from both domains. Our novel architecture results in a model that outperforms the state-of-the-art on a range of unsupervised domain adaptation scenarios and additionally produces visualizations of the private and shared representations enabling interpretation of the domain adaptation process.

- Saito, Kuniaki, Yoshitaka Ushiku, and Tatsuya Harada. "Asymmetric tri-training for unsupervised domain adaptation." Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.

  **Abstract:** Deep-layered models trained on a large number of labeled samples boost the accuracy of many tasks. It is important to apply such models to dif-ferent domains because collecting many labeled samples in various domains is expensive. In un-supervised domain adaptation, one needs to train a classifier that works well on a target domain when provided with labeled source samples and unlabeled target samples. Although many meth-ods aim to match the distributions of source and target samples, simply matching the distribution cannot ensure accuracy on the target domain. To learn discriminative representations for the target domain, we assume that artificially labeling tar-get samples can result in a good representation. Tri-training leverages three classifiers equally to give pseudo-labels to unlabeled samples, but the method does not assume labeling samples gener-ated from a different domain. In this paper, we propose an asymmetric tri-training method for unsupervised domain adaptation, where we as-sign pseudo-labels to unlabeled samples and train neural networks as if they are true labels. In our work, we use three networksasymmetrically. By asymmetric, we mean that two networks are used to label unlabeled target samples and one net-work is trained by the samples to obtain target-discriminative representations. We evaluate our method on digit recognition and sentiment anal-ysis datasets. Our proposed method achieves state-of-the-art performance on the benchmark digit recognition datasets of domain adaptation.

- Haeusser, Philip, et al. "Associative domain adaptation." Proceedings of the IEEE International Conference on Computer Vision. 2017.

  **Abstract:** We propose "associative domain adaptation", a novel technique for end-to-end domain adaptation with neural networks, the task of inferring class labels for an unlabeled target domain based on the statistical properties of a labeled source domain. Our training scheme follows the paradigm that in order to effectively derive class labels for the target domain, a network should produce statistically domain invariant embeddings, while minimizing the classification error on the labeled source domain. We accomplish this by reinforcing "associations" between source and target data directly in embedding space. Our method can easily be added to any existing classification network with no structural and almost no computational overhead. We demonstrate the effectiveness of our approach on various benchmarks and achieve state-of-the-art results across the board with a generic convolutional neural network architecture not specifically tuned to the respective tasks. Finally, we show that the proposed association loss produces embeddings that are more effective for domain adaptation compared to methods employing maximum mean discrepancy as a similarity measure in embedding space.

- Luo, Zelun, et al. "Label efficient learning of transferable representations acrosss domains and tasks." Advances in Neural Information Processing Systems. 2017.

  **Abstract:** We propose a framework that learns a representation transferable across different domains and tasks in a data efficient manner. Our approach battles domain shift with a domain adversarial loss, and generalizes the embedding to novel task using a metric learning-based approach. Our model is simultaneously optimized on labeled source data and unlabeled or sparsely labeled data in the target domain. Our method shows compelling results on novel classes within a new domain even when only a few labeled examples per class are available, outperforming the prevalent fine-tuning approach. In addition, we demonstrate the effectiveness of our framework on the transfer learning task from image object recognition to video action recognition.

- Cariucci, Fabio Maria, et al. "Autodial: Automatic domain alignment layers." 2017 IEEE International Conference on Computer Vision (ICCV). IEEE, 2017.

  **Abstract:** Classifiers trained on given databases perform poorly when tested on data acquired in different settings. This is explained in domain adaptation through a shift among distributions of the source and target domains. Attempts to align them have traditionally resulted in works reducing the domain shift by introducing appropriate loss terms, measuring the discrepancies between source and target distributions, in the objective function. Here we take a different route, proposing to align the learned representations by embedding in any given network specific Domain Alignment Layers, designed to match the source and target feature distributions to a reference one. Opposite to previous works which define a priori in which layers adaptation should be performed, our method is able to automatically learn the degree of feature alignment required at different levels of the deep network. Thorough experiments on different public benchmarks, in the unsupervised setting, confirm the power of our approach.

- Motiian, Saeid, et al. "Few-shot adversarial domain adaptation." Advances in Neural Information Processing Systems. 2017.

  **Abstract:** This work provides a framework for addressing the problem of supervised domain adaptation with deep models. The main idea is to exploit adversarial learning to learn an embedded subspace that simultaneously maximizes the confusion between two domains while semantically aligning their embedding. The supervised setting becomes attractive especially when there are only a few target data samples that need to be labeled. In this few-shot learning scenario, alignment and separation of semantic probability distributions is difficult because of the lack of data. We found that by carefully designing a training scheme whereby the typical binary adversarial discriminator is augmented to distinguish between four different classes, it is possible to effectively address the supervised adaptation problem. In addition, the approach has a high “speed” of adaptation, i.e. it requires an extremely low number of labeled target training samples, even one per category can be effective. We then extensively compare this approach to the state of the art in domain adaptation in two experiments: one using datasets for handwritten digit recognition, and one using datasets for visual object recognition.

## 其他领域适应论文

- McClosky, David. "Any domain parsing: automatic domain adaptation for natural language parsing." (2010).

  **Abstract:** Current efforts in syntactic parsing are largely data-driven. These methods require labeled examples of syntactic structures to learn statistical patterns governing these structures. Labeled data typically requires expert annotators which makes it both time consuming and costly to produce. Furthermore, once training data has been created for one textual domain, portability to similar domains is limited. This domain-dependence has inspired a large body of work since syntactic parsing aims to capture syntactic patterns across an entire language rather than just a specific domain.
The simplest approach to this task is to assume that the target domain is essentially the same as the source domain. No additional knowledge about the target domain is included. A more realistic approach assumes that only raw text from the target domain is available. This assumption lends itself well to semi-supervised learning methods since these utilize both labeled and unlabeled examples.
This dissertation focuses on a family of semi-supervised methods called self-training. Self-training creates semi-supervised learners from existing supervised learners with minimal effort. We first show results on self-training for constituency parsing within a single domain. While self-training has failed here in the past, we present a simple modification which allows it to succeed, producing state-of-the-art results for English constituency parsing. Next, we show how self-training is beneficial when parsing across domains and helps further when raw text is available from the target domain. One of the remaining issues is that one must choose a training corpus appropriate for the target domain or performance may be severely impaired. Humans can do this in some situations, but this strategy becomes less practical as we approach larger data sets. We present a technique, Any Domain Parsing, which automatically detects useful source domains and mixes them together to produce a customized parsing model. The resulting models perform almost as well as the best seen parsing models (oracle) for each target domain. As a result, we have a fully automatic syntactic constituency parser which can produce high-quality parses for all types of text, regardless of domain.

- Conneau, Alexis, et al. "Word translation without parallel data." arXiv preprint arXiv:1710.04087 (2017).

  **Abstract:** State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character informa-tion, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available

- Chen, Xinchi, et al. "Adversarial multi-criteria learning for chinese word segmentation." arXiv preprint arXiv:1704.07556 (2017).

  **Abstract:** Different linguistic perspectives causes many diverse segmentation criteria for Chinese word segmentation (CWS). Most existing methods focus on improve the performance for each single criterion. However, it is interesting to exploit these different criteria and mining their com-mon underlying knowledge. In this pa-per, we propose adversarial multi-criteria learning for CWS by integrating shared knowledge from multiple heterogeneous segmentation criteria. Experiments on eight corpora with heterogeneous segmen-tation criteria show that the performance of each corpus obtains a significant im-provement, compared to single-criterion learning. Source codes of this paper are available on Github.
- Zhang, Meng, et al. "Adversarial training for unsupervised bilingual lexicon induction." Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2017.

  **Abstract:** Word embeddings are well known to cap-ture linguistic regularities of the language on which they are trained. Researchers also observe that these regularities can transfer across languages. However, previ-ous endeavors to connect separate mono-lingual word embeddings typically require cross-lingual signals as supervision, either in the form of parallel corpus or seed lex-icon. In this work, we show that such cross-lingual connection can actually be established without any form of supervi-sion. We achieve this end by formulating the problem as a natural adversarial game, and investigating techniques that are cru-cial to successful training. We carry out evaluation on the unsupervised bilingual lexicon induction task. Even though this task appears intrinsically cross-lingual,we are able to demonstrate encouraging per-formance without any cross-lingual clues. 

- Mou, Lili, et al. "How transferable are neural networks in nlp applications?." arXiv preprint arXiv:1603.06111 (2016).

  **Abstract:** Transfer learning is aimed to make use of valuable knowledge in a source domain to help model performance in a target domain. It is particularly important to neural networks, which are very likely to be overfitting. In some fields like image processing, many studies have shown the effectiveness of neural network-based transfer learning. For neural NLP, however, existing studies have only casually applied transfer learning, and conclusions are inconsistent. In this paper, we conduct systematic case studies and provide an illuminating picture on the transferability of neural networks in NLP.

- Ruder, Sebastian, and Barbara Plank. "Strong baselines for neural semi-supervised learning under domain shift." arXiv preprint arXiv:1804.09530 (2018).

  **Abstract:** Novel neural models have been proposed in recent years for learning under domain shift. Most models, however, only evaluate on a single task, on proprietary datasets, or compare to weak baselines, which makes comparison of models difficult. In this paper, we re-evaluate classic general-purpose bootstrapping approaches in the context of neural networks under domain shifts vs. recent neural approaches and propose a novel multi-task tri-training method that reduces the time and space complexity of classic tri-training. Extensive experiments on two benchmarks are negative: while our novel method establishes a new state-of-the-art for sentiment analysis, it does not fare consistently the best. More importantly, we arrive at the somewhat surprising conclusion that classic tri-training, with some additions, outperforms the state of the art. We conclude that classic approaches constitute an important and strong baseline.

- Zhang, Yuan, Regina Barzilay, and Tommi Jaakkola. "Aspect-augmented adversarial networks for domain adaptation." Transactions of the Association for Computational Linguistics 5 (2017): 515-528.

  **Abstract:** We introduce a neural method for transfer learning between two (source and target) classification tasks or aspects over the same domain. Rather than training on target labels, we use a few keywords pertaining to source and target aspects indicating sentence relevance instead of document class labels. Documents are encoded by learning to embed and softly select relevant sentences in an aspect-dependent manner. A shared classifier is trained on the source encoded documents and labels, and applied to target encoded documents. We ensure transfer through aspect-adversarial training so that encoded documents are, as sets, aspect-invariant. Experimental results demonstrate that our approach outperforms different baselines and model variants on two datasets, yielding an improvement of 27% on a pathology dataset and 5% on a review dataset.

## ACL2019
- Semi-supervised Domain Adaptation for Dependency Parsing
  Zhenghua Li, Xue Peng, Min Zhang, Rui Wang and Luo Si

- Task Refinement Learning for Improved Accuracy and Stability of Unsupervised Domain Adaptation
  Yftah Ziser and Roi Reichart

- Automatic Domain Adaptation Outperforms Manual Domain Adaptation for Predicting Financial Outcomes
  Marina Sedinkina, Nikolas Breitkopf and Hinrich Schütze

- Reinforced Training Data Selection for Domain Adaptation
  Miaofeng Liu, Yan Song, Hongbin Zou and Tong Zhang

- Automatic Generation of High Quality CCGbanks for Parser Domain Adaptation
  Masashi Yoshikawa, Hiroshi Noji, Koji Mineshima and Daisuke Bekki

- A Cross-Domain Transferable Neural Coherence Model
  Peng Xu, Hamidreza Saghir, Jin Sung Kang, Teng Long, Avishek Joey Bose, Yanshuai Cao and Jackie Chi Kit Cheung

- Semi-supervised Stochastic Multi-Domain Learning using Variational Inference
  Yitong Li, Timothy Baldwin and Trevor Cohn

- Dynamically Composing Domain-Data Selection with Clean-Data Selection by ``Co-Curricular Learning" for Neural Machine Translation
  Wei Wang, Isaac Caswell and Ciprian Chelba

- Neural Temporality Adaptation for Document Classification: Diachronic Word Embeddings and Domain Adaptation Models
  Xiaolei Huang and Michael J. Paul

- SParC: Cross-Domain Semantic Parsing in Context
  Tao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria Lin, Suyi Li, Heyang Er, Irene Li, Bo Pang, Tao Chen, Emily Ji, Shreya Dixit, David Proctor, Sungrok Shim, Jonathan Kraft, Vincent Zhang, Caiming Xiong, Richard Socher and Dragomir Radev

- Cross-domain and Cross-lingual Abusive Language Detection: a Hybrid Approach with Deep Learning and a Multilingual Lexicon
  Endang Wahyu Pamungkas and Viviana Patti

- Robust Zero-Shot Cross-Domain Slot Filling with Example Values
  Darsh Shah, Raghav Gupta, Amir Fayazi and Dilek Hakkani-Tur

- Reversing Gradients in Adversarial Domain Adaptation for Question Deduplication and Textual Entailment Tasks
  Anush Kamath, Sparsh Gupta and Vitor Carvalho

- Domain Adaptive Inference for Neural Machine Translation
  Danielle Saunders, Felix Stahlberg, Adrià de Gispert and Bill Byrne

- Adversarial Domain Adaptation Using Artificial Titles for Abstractive Title Generation
  Francine Chen and Yan-Ying Chen

- Domain Adaptation of Neural Machine Translation by Lexicon Induction
  Junjie Hu, Mengzhou Xia, Graham Neubig and Jaime Carbonell

- Cross-Domain NER using Cross-Domain Language Modeling
  Chen Jia, Xiaobo Liang and Yue Zhang

- Towards Complex Text-to-SQL in Cross-Domain Database with Intermediate Representation
  Jiaqi Guo, Zecheng Zhan, Yan Gao, Yan Xiao, Jian-Guang LOU, Ting Liu and Dongmei Zhang

- Handling Domain Shift in Coreference Evaluation by Using Automatically Extracted Minimum Spans
  Nafise Sadat Moosavi, Leo Born, Massimo Poesio and Michael Strube

- Domain Adaptive Dialog Generation via Meta Learning
  Kun Qian and Zhou Yu

- Multi-Source Cross-Lingual Model Transfer: Learning What to Share
  Xilun Chen, Ahmed Hassan Awadallah, Hany Hassan, Wei Wang and Claire Cardie

- Dual Adversarial Neural Transfer for Low-Resource Named Entity Recognition
  Joey Tianyi Zhou, Hao Zhang, Di Jin, Hongyuan Zhu, Meng Fang, Rick Siow Mong Goh and Kenneth Kwok

- Learning Transferable Feature Representations Using Neural Networks
  Himanshu Sharad Bhatt, Shourya Roy, Arun Rajkumar and Sriranjani Ramakrishnan

- Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies
  Yunsu Kim, Yingbo Gao and Hermann Ney

- Low-resource Deep Entity Resolution with Transfer and Active Learning
  Jungo Kasai, Kun Qian, Sairam Gurajada, Yunyao Li and Lucian Popa

- Massively Multilingual Transfer for NER
  Afshin Rahimi, Yuan Li and Trevor Cohn

- Cross-Lingual Syntactic Transfer through Unsupervised Adaptation of Invertible Projections
  Junxian He, Zhisong Zhang, Taylor Berg-Kirkpatrick and Graham Neubig

- Soft Representation Learning for Sparse Transfer
  Haeju Park, Jinyoung Yeo, Gengyu Wang and Seung-won Hwang

- Hierarchical Transfer Learning for Multi-label Text Classification
 Siddhartha Banerjee, Cem Akkaya, Francisco Perez-Sorrosal and Kostas Tsioutsiouliklis

- Variational Pretraining for Semi-supervised Text Classification
  Suchin Gururangan, Tam Dang, Dallas Card and Noah A. Smith

## NAACL2019
- Adversarial Category Alignment Network for Cross-domain Sentiment Classification
  Xiaoye Qu, Zhikang Zou, Yu Cheng, Yang Yang and Pan Zhou

- Curriculum Learning for Domain Adaptation in Neural Machine Translation
  Xuan Zhang, Pamela Shapiro, Gaurav Kumar, Paul McNamee, Marine Carpuat and Kevin Duh

- Domain adaptation for part-of-speech tagging of noisy user-generated text
  Luisa Berlanda, Dietrich Trautmann and Benjamin Roth

- Improving Cross-Domain Chinese Word Segmentation with Word Embeddings
  Yuxiao Ye, Weikang Li, Yue Zhang, Likun Qiu and Jian Sun

- Improving Domain Adaptation Translation with Shared Encoder-Decoder
  Shuhao Gu, Yang Feng and Qun Liu

- Joint Learning of Pre-Trained and Random Units for Domain Adaptation in Part-of-Speech Tagging
  Sara Meftah, Youssef Tamaazousti, Nasredine Semmar, Hassane Essafi and Fatiha Sadat

- Multilingual prediction of Alzheimer’s disease through domain adaptation and concept-based language modelling
  Kathleen C. Fraser, Nicklas Linz, Bai Li, Kristina Lundholm Fors, Frank Rudzicz, Alexandra Konig, Jan Alexandersson, Philippe Robert and Dimitrios Kokkinakis

- NLP Whack-A-Mole: Challenges in Cross-Domain Temporal Expression Extraction
  Amy Olex, Luke Maffey and Bridget McInnes

- Overcoming Catastrophic Forgetting During Domain Adaptation of Neural Machine Translation
  Brian Thompson, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh and Philipp Koehn

- Simplified Neural Unsupervised Domain Adaptation
  Timothy Miller

- Active Learning for New Domains in Natural Language Understanding
  Stanislav Peshterliev, John Kearney, Abhyuday Jagannatha, Imre Kiss and Spyros Matsoukas

- Locale-agnostic Universal Domain Classification Model in Spoken Language Understanding
  Jihwan Lee, Ruhi Sarikaya and Young-Bum Kim

- Robust Semantic Parsing with Adversarial Learning for Domain Generalization
  Gabriel Marzinotto, Geraldine Damnati, Frederic Bechet and Benoit Favre

- An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models
  Alexandra Chronopoulou, Christos Baziotis and Alexandros Potamianos

- Bayesian Analysis of Crosslingual Transfer Learning in Probabilistic Topic Models
  Shudong Hao and Michael J. Paul

- Cross-lingual Multi-Level Adversarial Transfer to Enhance Low-Resource Name Tagging
  Lifu Huang, Heng Ji and Jonathan May

- Linguistic Knowledge and Transferability of Contextual Representations
  Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew Peters and Noah A. Smith

- Low-Resource Syntactic Transfer with Unsupervised Source Reordering
  Mohammad Sadegh Rasooli and Michael Collins

- On Difficulties of Cross-Lingual Transfer with Order Differences: A Case Study on Dependency Parsing
  Wasi Ahmad, Zhisong Zhang, Xuezhe Ma, Eduard Hovy, Kai-Wei Chang and Nanyun Peng

- Polyglot Contextual Representations Improve Crosslingual Transfer
  Phoebe Mulcaire, Jungo Kasai and Noah A. Smith

- Transferable Neural Projection Representations
  Chinnadhurai Sankar, Sujith Ravi and Zornitsa Kozareva

- Cross-lingual Transfer Learning for Japanese Named Entity Recognition
  Andrew Johnson, Penny Karanasou, Judith Gaspers and Dietrich Klakow

## ACL Anthology 2017,2018
- Neural Adaptation Layers for Cross-domain Named Entity Recognition

  **Abstract** Recent research efforts have shown that neural architectures can be effective in conventional information extraction tasks such as named entity recognition, yielding state-of-the-art results on standard newswire datasets. However, despite significant resources required for training such models, the performance of a model trained on one domain typically degrades dramatically when applied to a different domain, yet extracting entities from new emerging domains such as social media can be of significant interest. In this paper, we empirically investigate effective methods for conveniently adapting an existing, well-trained neural NER model for a new domain. Unlike existing approaches, we propose lightweight yet effective methods for performing domain adaptation for neural models. Specifically, we introduce adaptation layers on top of existing neural architectures, where no re-training using the source domain data is required. We conduct extensive empirical studies and show that our approach significantly outperforms state-of-the-art methods.

- Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification
  
  **Abstract** We consider the cross-domain sentiment classification problem, where a sentiment classifier is to be learned from a source domain and to be generalized to a target domain. Our approach explicitly minimizes the distance between the source and the target instances in an embedded feature space. With the difference between source and target minimized, we then exploit additional information from the target domain by consolidating the idea of semi-supervised learning, for which, we jointly employ two regularizations — entropy minimization and self-ensemble bootstrapping — to incorporate the unlabeled target data for classifier refinement. Our experimental results demonstrate that the proposed approach can better leverage unlabeled data from the target domain and achieve substantial improvements over baseline methods in various experimental settings.

- The BQ Corpus: A Large-scale Domain-specific Chinese Corpus For Sentence Semantic Equivalence Identification
 
  **Abstract** This paper introduces the Bank Question (BQ) corpus, a Chinese corpus for sentence semantic equivalence identification (SSEI). The BQ corpus contains 120,000 question pairs from 1-year online bank custom service logs. To efficiently process and annotate questions from such a large scale of logs, this paper proposes a clustering based annotation method to achieve questions with the same intent. First, the deduplicated questions with the same answer are clustered into stacks by the Word Mover’s Distance (WMD) based Affinity Propagation (AP) algorithm. Then, the annotators are asked to assign the clustered questions into different intent categories. Finally, the positive and negative question pairs for SSEI are selected in the same intent category and between different intent categories respectively. We also present six SSEI benchmark performance on our corpus, including state-of-the-art algorithms. As the largest manually annotated public Chinese SSEI corpus in the bank domain, the BQ corpus is not only useful for Chinese question semantic matching research, but also a significant resource for cross-lingual and cross-domain SSEI research. The corpus is available in public.

- Transferring from Formal Newswire Domain with Hypernet for Twitter POS Tagging

  **Abstract** Part-of-Speech (POS) tagging for Twitter has received considerable attention in recent years. Because most POS tagging methods are based on supervised models, they usually require a large amount of labeled data for training. However, the existing labeled datasets for Twitter are much smaller than those for newswire text. Hence, to help POS tagging for Twitter, most domain adaptation methods try to leverage newswire datasets by learning the shared features between the two domains. However, from a linguistic perspective, Twitter users not only tend to mimic the formal expressions of traditional media, like news, but they also appear to be developing linguistically informal styles. Therefore, POS tagging for the formal Twitter context can be learned together with the newswire dataset, while POS tagging for the informal Twitter context should be learned separately. To achieve this task, in this work, we propose a hypernetwork-based method to generate different parameters to separately model contexts with different expression styles. Experimental results on three different datasets show that our approach achieves better performance than state-of-the-art methods in most cases.

- Bootstrap Domain-Specific Sentiment Classifiers from Unlabeled Corpora

  **Abstract** There is often the need to perform sentiment classification in a particular domain where no labeled document is available. Although we could make use of a general-purpose off-the-shelf sentiment classifier or a pre-built one for a different domain, the effectiveness would be inferior. In this paper, we explore the possibility of building domain-specific sentiment classifiers with unlabeled documents only. Our investigation indicates that in the word embeddings learned from the unlabeled corpus of a given domain, the distributed word representations (vectors) for opposite sentiments form distinct clusters, though those clusters are not transferable across domains. Exploiting such a clustering structure, we are able to utilize machine learning algorithms to induce a quality domain-specific sentiment lexicon from just a few typical sentiment words (“seeds”). An important finding is that simple linear model based supervised learning algorithms (such as linear SVM) can actually work better than more sophisticated semi-supervised/transductive learning algorithms which represent the state-of-the-art technique for sentiment lexicon induction. The induced lexicon could be applied directly in a lexicon-based method for sentiment classification, but a higher performance could be achieved through a two-phase bootstrapping method which uses the induced lexicon to assign positive/negative sentiment scores to unlabeled documents first, a nd t hen u ses those documents found to have clear sentiment signals as pseudo-labeled examples to train a document sentiment classifier v ia supervised learning algorithms (such as LSTM). On several benchmark datasets for document sentiment classification, our end-to-end pipelined approach which is overall unsupervised (except for a tiny set of seed words) outperforms existing unsupervised approaches and achieves an accuracy comparable to that of fully supervised approaches.

- Adversarial Domain Adaptation for Duplicate Question Detection

  **Abstract** We address the problem of detecting duplicate questions in forums, which is an important step towards automating the process of answering new questions. As finding and annotating such potential duplicates manually is very tedious and costly, automatic methods based on machine learning are a viable alternative. However, many forums do not have annotated data, i.e., questions labeled by experts as duplicates, and thus a promising solution is to use domain adaptation from another forum that has such annotations. Here we focus on adversarial domain adaptation, deriving important findings about when it performs well and what properties of the domains are important in this regard. Our experiments with StackExchange data show an average improvement of 5.6% over the best baseline across multiple pairs of domains.

- Multi-Source Domain Adaptation with Mixture of Experts

  **Abstract** We propose a mixture-of-experts approach for unsupervised domain adaptation from multiple sources. The key idea is to explicitly capture the relationship between a target example and different source domains. This relationship, expressed by a point-to-set metric, determines how to combine predictors trained on various domains. The metric is learned in an unsupervised fashion using meta-training. Experimental results on sentiment analysis and part-of-speech tagging demonstrate that our approach consistently outperforms multiple baselines and can robustly handle negative transfer.

Identifying Domain Adjacent Instances for Semantic Parsers

  **Abstract** When the semantics of a sentence are not representable in a semantic parser’s output schema, parsing will inevitably fail. Detection of these instances is commonly treated as an out-of-domain classification problem. However, there is also a more subtle scenario in which the test data is drawn from the same domain. In addition to formalizing this problem of domain-adjacency, we present a comparison of various baselines that could be used to solve it. We also propose a new simple sentence representation that emphasizes words which are unexpected. This approach improves the performance of a downstream semantic parser run on in-domain and domain-adjacent instances.

A Pilot Study of Domain Adaptation Effect for Neural Abstractive Summarization

  **Abstract** We study the problem of domain adaptation for neural abstractive summarization. We make initial efforts in investigating what information can be transferred to a new domain. Experimental results on news stories and opinion articles indicate that neural summarization model benefits from pre-training based on extractive summaries. We also find that the combination of in-domain and out-of-domain setup yields better summaries when in-domain data is insufficient. Further analysis shows that, the model is capable to select salient content even trained on out-of-domain data, but requires in-domain data to capture the style for a target domain.

Cross-Lingual Transfer of Semantic Roles: From Raw Text to Semantic Roles

  **Abstract** We describe a transfer method based on annotation projection to develop a dependency-based semantic role labeling system for languages for which no supervised linguistic information other than parallel data is available. Unlike previous work that presumes the availability of supervised features such as lemmas, part-of-speech tags, and dependency parse trees, we only make use of word and character features. Our deep model considers using character-based representations as well as unsupervised stem embeddings to alleviate the need for supervised features. Our experiments outperform a state-of-the-art method that uses supervised lexico-syntactic features on 6 out of 7 languages in the Universal Proposition Bank.



































